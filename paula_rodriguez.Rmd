---
title: "Práctica final"
subtitle: "Módulo de Estadística y R"
author: "Paula Rodríguez Martínez"
date: "`r format(Sys.time(), '%d-%m-%Y')`"
output: html_document
---

```{r setup, include=FALSE}
# No toquéis esta parte
knitr::opts_chunk$set(echo = TRUE, warning= FALSE)
set.seed(1234)
```


# Práctica

## Librerías

```{r}
# Poned las librerías que uséis aquí
library(dplyr)
library(ggplot2)
library(gmodels)
library(readxl)
library(tidyverse)
library(corrplot)
library(rsample)
library(caret)
library(MASS)
library(dials)
library(parsnip)
library(recipes)
library(workflows)
library(doParallel)
library(tune)
library(yardstick)
library(ranger)

```

## Introducción

Se pretende identificar ineficiencias en el proceso de venta o factores internos y externos que pueden estar impactando el rendimiento de las mismas de una empresa de dispositivos electrónicos para las franquicias de todo el país.

Las variables son las siguientes:

1.	Rentabieco = Rentabilidad económica
2.	Rentabifin = Rentabilidad financier
3.	Endp = Nivel de endeudamiento de las franquicias que venden estos dispositivos, valorado en euros
4.	Liq = Liquidez monetaria de las franquicias
5.	Productividad = ratio. Buscar la relación que puede tener con otras variables.
6.	Coe = cuota de mercado
7.	Edad = tiempo que llevan las franquicias operando
8.	Conce = número de entidades que dan este mismo servicio a nivel municipio
9.	Numac = nº accionistas
10.	Numpa = nº participados
11.	Numest = nº establecimientos por empresa
12.	Estp = nº establecimientos en otras provincias


## Ejercicio 0

Teniendo en cuenta la descripción de cada variable, y los valores que véis en éstas, formatead correctamente el dataframe. 

Cargamos el dataframe de datos en la variable df:  


```{r}
df <- read_xlsx("DatosPractica.xlsx")
str(df)
```
  
Eliminamos las variables Registro, grupo y fju ya que en este análisis no son necesarias.
Convertimos a factor la variable estp de forma que "No" se refiere a que no hay establecimientos en otras provincias.

```{r}
df <- df %>% 
  mutate(
    estp=factor(estp, levels = 0:1, labels = c("No", "Yes"))
    )
df <- df %>%       
    dplyr::select(-c(REGISTRO, grupo, fju))
str(df)
```
  
Calculamos el número de missings y observamos que no hay por lo que podemos seguir con el estudio inicial del dataframe:

```{r}
df %>% map_dbl(.f = function(x){sum(is.na(x))})
```

 
Para el desarrollo de este estudio se sugiere seguir las siguientes instrucciones:

## Ejercicio 1

Análisis descriptivo. Estudiar la distribución de las variables, así como detectar posibles valores atípicos o relaciones.

1.Variable rentabieco:  

Se corresponde a la rentabilidad económica de las tiendas que venden los dispositivos electrónicos y es una variable cuantitativa continua. Para analizarla usaremos los gráficos correspondientes.
```{r}
par(mfrow = c(1, 2))
hist(df$rentabieco)
boxplot(df$rentabieco)
```
  
  Podemos observar que la distribución se asemeja a una normal asimétrica y leptocúrtica con media próxima a cero y con una mediana también cercana pero superior al cero. Sin embargo, sin hacer el shapiro test no se puede asegurar que sea una distribución normal. Además, hay muchos datos atípicos que van desde más del 150 de rentabilidad económica negativa hasta 50 de rentabilidad positiva pero se ve claramente que el 50% de los datos se agrupan en torno al rango de 0 a 10 de rentabilidad económica. Comprobamos con la función summary si es correcta las suposiciones:
  
```{r}
summary(df$rentabieco)
```
  
Efectivamente vemos outliers que tienen de valor -179 en mínimo y en máximo de 48 y que el 50% de los datos está entre el 0.129 y el 5 aproximadamente.  



2. Variable rentabifin:  

Variable cuantitativa continua que representa la rentabilidad financiera de dichas tiendas. De igual manera que en la variable anterior, la distribución se aproxima a una normal casi simétrica y leptocúrtica pero con una media y mediana superior a cero y valores atípicos de hasta 400 tanto positivos como negativos.

```{r}
par(mfrow = c(1, 2))
hist(df$rentabifin)
boxplot(df$rentabifin)
```
   
Los valores exactos de esta variable son los siguientes:


```{r}
summary(df$rentabifin)
```
  
3. Variable endp:  

Se corresponde al nivel de endeudamiento en euros que tienen las tiendas que venden los dispositivos electrónicos. Es una variable cuantitativa continua positiva donde la media y mediana giran en torno al nivel 70 con datos atípicos a partir del 150 aproximadamente. Con el histograma no se puede deducir que se parezca a una normal la distribución.



```{r}

par(mfrow = c(1, 2))
hist(df$endp)
boxplot(df$endp)
```

Los datos estadísticos de la distribución son:
```{r}
summary(df$endp)
```
  
    
4.Variable Liq:  

Variable que recoge la liquidez monetaria de las tiendas que venden los dispositivos. La liquidez es el cociente entre el activo corriente y el pasivo corriente por lo que será un número superior a 0, continua y cuantitativa. En el histograma observamos que los datos no se distribuyen según una normal y, que probablemente, para un mayor análisis debamos escalarlo de forma logarítmica consiguiendo una mejor distribución de los datos. En el diagrama de caja y bigotes se pueden ver que hay muchos outliers y que el 50% de los datos se encuentran entre el 0 y el 5 como mucho.
  
```{r}
par(mfrow = c(1, 2))
hist(df$liq)
boxplot(df$liq)
```
  
Los datos estadísticos de esta variable son los siguientes:
```{r}
summary(df$liq)
```

  
5. Variable productividad:    

Se corresponde con el ratio de productividad que tienen las franquicias. En el histograma la mayor parte de los datos se agrupan en torno a productividad 0 y 25, y al igual que la variable anterior, recomendaría un cambio de variable de tipo logaritmo para mejorar la escala de los gráficos. De nuevo, vemos mucha cantidad de outliers a partir del dato 50 hasta el 250 y alguno incluso negativo muy cercano al 0.


```{r}
par(mfrow = c(1, 2))
hist(df$PRODUCTIVIDAD)
boxplot(df$PRODUCTIVIDAD)
```

Los valores estadísticos de dicha distribución son:
```{r}
summary(df$PRODUCTIVIDAD)
```
  
6. Variable COE:   

La cuota de mercado es una variable continua, positiva y cuantitativa. Sus datos varían entre el 0 y el 0,5 por lo que estamos hablando del ratio de la cuota de mercado y que se agrupa más del 50% de los datos desde el 0 hasta el 0.02 o 0.03 a simple vista. A partir de esa cifra encontramos outliers hasta el 0.5. La distribución no parece ser normal, sin embargo, debido a la distancia que hay entre los datos de nuevo habría que escalar la variable logarítmicamente.

```{r}
par(mfrow = c(1, 2))
hist(df$coe)
boxplot(df$coe)
```
Los datos estadísticos son los siguientes:
```{r}
summary(df$coe)
```
  
    
7. Variable edad:  

Se corresponde con la edad que llevan las tiendas activas vendiendo dispositivos electrónicos por lo que será una variable continua, positiva y cuantitativa. La variable no se asemeja a una distribución normal y no es necesario escalarla. Los datos están mejor distribuidos que en los anteriores casos puesto que hay menos outliers y se refieren a las tiendas que más tiempo llevan operando como es el caso de 50 años. En el histograma podemos deducir que la mayoría de las tiendas llevan operando entre 0 años y 15 años y la mediana se ve claramente que está en torno a los 11 años.

```{r}
par(mfrow = c(1, 2))
hist(df$edad)
boxplot(df$edad)
```
  
Los datos estadísticos son:
```{r}
summary(df$edad)
```
  
    
8. Variable conce:   

Se corresponde con el número de concesionarios que hay en cada municipio por lo que será una variable discreta, cuantitativa y positiva. La mayoría de municipios tienen entre 0 y 20 concesionarios, siendo valores atípicos los municipios con 65,  150 concesionarios y 230 aproximadamente aproximadamente. La mediana según el diagrama de cajas y bigotes está en torno a 10 concesionarios y la media debe ser un poco superior al verse modificada por los outliers.

```{r}
par(mfrow = c(1, 2))
hist(df$conce)
boxplot(df$conce)
```
Los datos estadísticos son:

```{r}
summary(df$conce)
```
  
9. Variable numac:   

El número de accionistas que participan en la franquicia o tienda es una variable positiva, discreta y cuantitativa. No se corresponde con una variable normal y encontramos valores atípicos a partir del número 5 de accionistas hasta el 15 aproximadamente. Observamos que la mayoría de empresas (el 75% de ellas) tienen entre 0 y 2 accionista siendo por ello la mediana 1 como vemos en el diagrama de cajas y bigotes. La media debido a los outliers se verá desplazada a un valor superior a 1.

```{r}
par(mfrow = c(1, 2))
hist(df$numac)
boxplot(df$numac)
```
  
Los datos estadísticos que confirman nuestras suposiciones son:
```{r}
summary(df$numac)
```
  
10. Variable numpa:   

Se corresponde a los participantes de la empresa por lo que será una variable positiva, discreta y cuantitativa. En el histograma observamos que todas las empresas, a excepción de las que tienen datos atípicos, tienen 0 participantes. Así, la mediana será 0 y la media será un poco superior a 0 debido a ese desplazamiento que mencioné previamente. Tenemos outliers hasta el número 20 de participantes.


```{r}
par(mfrow = c(1, 2))
hist(df$numpa)
boxplot(df$numpa)
```
Los datos estadísticos son:   

```{r}
summary(df$numpa)
```
  
11. Variable numest: 

Representa el número de establecimientos que tiene la empresa por lo que, de nuevo, la variable será positiva, discreta y cuantitativa. Por el diagrama de cajas y bigotes, observamos que la variable comienza en el 1 y que se corresponde con la mediana por lo que el 50% de los datos tienen un establecimiento únicamente. El otro 25% se corresponde con dos establecimientos y el último 25% con 3 establecimientos. A partir de 4 establecimientos incluido, son datos atípicos y su máximo es 34 establecimientos. Con el histograma observamos que no sigue una distribución normal y que la mediana caerá en un establecimiento. 

```{r}
par(mfrow = c(1, 2))
hist(df$numest)
boxplot(df$numest)
```  
Los datos estadísticos son:
```{r}
summary(df$numest)
```
  

12. Variable estp:  

Representa si tiene establecimientos en otras provincias y es una variable categórica donde el 1 representa que sí tiene establecimientos fuera y el 0 que no tiene. Para ello, transformamos dicha variable a factor siguiendo el criterio que hemos elegido.
```{r}
plot(df$estp)
``` 
  
Al ser una variable categórica, hacemos una tabla de contingencia para ver el número exacto de empresas que tienen tiendas en otras provincias:
```{r}
table(df$estp)
```
  
Una vez que hemos estudiado cada variable por separado, haremos las transformaciones correspondientes y estudiaremos las relaciones entre ambas. Transformaremos logarítmicamente las variables endp,liq, PRODUCTIVIDAD, coe y conce como mencionamos. Como tenemos variables que toman el valor 0, desplazaremos los datos 1 o 1.2 en positivo para que no se produzcan datos NaN.

```{r}
df <- df %>% 
  mutate(
    endp=log(endp+1),
    liq=log(liq+1),
    PRODUCTIVIDAD=log(PRODUCTIVIDAD+1.2),
    coe=log(coe+1),
    conce=log(conce+1)
    )
```

Creamos un dataframe con las variables continuas:
```{r}
dfc<- df %>% 
  dplyr::select(c(-PROVINCIA, -estp))
str(dfc)
```
Calculamos las correlaciones de las variables continuas:

```{r}
corrplot(cor(dfc), method = "circle",type="upper")
```
  
Tenemos una correlación significativa entre muchas variables pero a destacar por ejemplo endp con liq es negativa y muy significativa o por ejemplo el numero de empleados con el numest que es positiva y también muy significativa. El estudio con ventas se hará en el apartado siguiente.  

## Ejercicio 2

Análisis del Comportamiento de las Ventas y Variables que le Afectan.

La variable dependiente Ventas es una variable continua, positiva y cuantitativa. 
```{r}
par(mfrow = c(1, 2))
hist(df$VENTAS)
boxplot(df$VENTAS)
```
  
Los datos estadísticos de la variable ventas son:
```{r}
summary(df$VENTAS)
```

Para el estudio de cada variable con ventas lo primero que debemos hacer es ver si son distribuciones normales y después aplicar los respectivos test adecuadamente. El test de normalidad para una variable continua es el saphiro.test y la hipótesis nula es que la variable tiene una distribución normal. Usaremos un 95% de confianza por lo que solo aceptaremos la hipótesis nula si el valor de p es mayor que el 5%. Con este criterio, vemos que ninguna variable cumple con los requisitos de la hipótesis nula por lo que la rechazamos y podemos decir que las variables no tienen una distribución normal. 
```{r}
sapply(dfc, shapiro.test)
```  
  
Calculamos la correlación entre ventas y las variables continuas. La hipótesis nula será que no existe una relación significativa entre las variables. Si p es menor que el 5% rechazaremos la hipótesis nula y veremos que tienen una relación significativa con ventas y viene dada por el coeficiente de correlación que es el siguiente. Para calcular la relación usaremos el test de correlación con el método de Spearman ya que las variables no son normales. Cabe destacar que todas las p calculadas son menores que 0.05 por lo que las variables tienen relación con ventas.


```{r}
cor(df$VENTAS, dfc,method="spearman")

```


Veamos un análisis individual de cada relación: 


La variable rentabieco tiene una correlación positiva no muy elevada de 0.235 con ventas. En el gráfico observamos que a mayor venta menor es la rentabilidad económica.
```{r}
cor.test(df$VENTAS, df$rentabieco, method = "spearman")
```

```{r}
ggplot(data = df, aes(x = rentabieco, y = VENTAS)) +
  geom_point()
```
  
La variable rentabifin tiene una correlación negativa y menos significativa de -0.05 con las ventas lo que da que pensar que puede que rentabifin sea redundante en nuestro modelo. 
```{r}
cor.test(df$VENTAS, df$rentabifin, method = "spearman")
```
```{r}
ggplot(data = df, aes(x = rentabifin, y = VENTAS)) +
  geom_point()
```  

La variable endp tiene una correlación negativa y relevante de -0.285 con ventas. El gráfico de ambas variables es:
```{r}
cor.test(df$VENTAS, df$endp, method = "spearman")
```
```{r}
ggplot(data = df, aes(x = endp, y = VENTAS)) +
  geom_point()
```
  
La variable liq tiene una correlación positiva pero no muy relevante de 0.135. El gráfico de ambas variables es el siguiente:
```{r}
cor.test(df$VENTAS, df$liq, method = "spearman")
```
```{r}
ggplot(data = df, aes(x = liq, y = VENTAS)) +
  geom_point()
```
  
La variable productividad tiene una correlación positiva y significativa de 0.49, esto quiere decir que la productividad está relacionada con las ventas lo cual tiene sentido ya que cuanto más vendas debería producirse más.
```{r}
cor.test(df$VENTAS, df$PRODUCTIVIDAD, method = "spearman")
```
```{r}
ggplot(data = df, aes(x = PRODUCTIVIDAD, y = VENTAS)) +
  geom_point()
```
  
La variable coe tiene una correlación positiva y muy significativa de 0.843 con la variable ventas. El gráfico de ambas variables es:
```{r}
cor.test(df$VENTAS, df$coe, method = "spearman")
```
```{r}
ggplot(data = df, aes(x = coe, y = VENTAS)) +
  geom_point()
```
  
La variable edad tiene también una correlación positiva pero menos significativa de 0.412 con las ventas. Esto se podría explicar pues puede llegar a ser lógico que las franquicias que más tiempo lleven es porque más venden pero no es muy relevante y solo es una suposición. El gráfico entre ambas variables es:
```{r}
cor.test(df$VENTAS, df$edad, method = "spearman")
```
```{r}
ggplot(data = df, aes(x = edad, y = VENTAS)) +
  geom_point()
```
  
La variable conce tiene una correlación positiva pero no tan relevante de 0.261 con las ventas. 
```{r}
cor.test(df$VENTAS, df$rentabieco, method = "spearman")
```
  
El gráfico de ambas es:
```{r}
ggplot(data = df, aes(x = conce, y = VENTAS)) +
  geom_point()
```
  
La variable numac tiene una correlación positiva y significativa de 0.54. El gráfico de ambas variables es el siguiente:
```{r}
cor.test(df$VENTAS, df$numac, method = "spearman")
```
```{r}
ggplot(data = df, aes(x = numac, y = VENTAS)) +
  geom_point()
```
  
La variable numpa tiene una correlación menor pero positiva de 0.325 por lo que también hay relación con las ventas de forma directa. El gráfico es:
```{r}
cor.test(df$VENTAS, df$numpa, method = "spearman")
```
```{r}
ggplot(data = df, aes(x = numpa, y = VENTAS)) +
  geom_point()
```
  
La variable numest tiene una correlación positiva de 0.425 por lo que de nuevo hay relación directa con las ventas y el número de establecimientos por empresa. El gráfico es:
```{r}
cor.test(df$VENTAS, df$numest, method = "spearman")
```
```{r}
ggplot(data = df, aes(x = numest, y = VENTAS)) +
  geom_point()
```

Por último, la variable NÚMERO DE EMPLEADOS es sin lugar a dudas la que más relación tiene con las ventas y esto es razonable puesto que a un mayor número de ventas habrá un mayor número de empleados en dicha tienda. En el gráfico vemos:

```{r}
cor.test(df$VENTAS, df$"NÚMERO DE EMPLEADOS", method = "spearman")
```
```{r}
plot(df$"NÚMERO DE EMPLEADOS",df$VENTAS)
```

Cabe destacar que ninguna variable tiene una relación tan significativa como para ver una relación directa lineal en los gráficos representados a excepción del número de empleados.

Para las variables categóricas tendremos que hacer uso de diferentes test según las características de cada una.   

La variable PROVINCIAS es una variable categórica de más de tres grupos y no es normal por lo que el test ha aplicar será el de Kruskall Wallis. Asumimos que la hipótesis nula es que no existe relación significativa entre ambas variables. Con el valor que tenemos de p no podemos rechazar la hipótesis nula por lo que la variable PROVINCIAS no tiene una relación significativa con las ventas. 



```{r}
kruskal.test(VENTAS ~ PROVINCIA, df)
```
  
El gráfico de ambas variables es:


```{r}
ggplot(data = df, aes(x = PROVINCIA, y = VENTAS)) +
  geom_point()
```

La variable estp es una variable categórica con distribución no normal y con dos únicos niveles "sí" y "no", por ello se debe usar el test de Mann-Whitney U. Con el p dado podemos rechazar la hipótesis nula y decir que sí hay una relación entre ambas variables.

```{r}
wilcox.test(VENTAS ~ estp, df)
```

El gráfico dado es:
```{r}
ggplot(data = df, aes(x = estp, y = VENTAS)) +
  geom_point()
```

## Ejercicio 3

Realizar una tabla de contingencia entre Ventas y Número de empleados, una vez categorizadas ambas variables, para número de empleados, por ejemplo, la utilizada a nivel Europeo, la cual divide a las empresas en Microempresas (1-9 trabajadores), Pequeña empresa (10-49 trabajadores), Mediana empresa (50-249 trabajadores) y Gran empresa (250 y más trabajadores)).

Categorizamos la variable de Número de Empleados:

```{r}
df$empresa <- cut(df$"NÚMERO DE EMPLEADOS", breaks = c(0, 9, 49, 249, Inf), labels = c("Microempresas", "Pequeña empresa", "Mediana empresa", "Gran empresa"))
```
  
Categorizamos la variable de Ventas en función de sus cuartiles pues con ellos ya tenemos hecha la división de ventas lo más exacta posible. Por tanto, crearemos las siguientes categorías:
de 0 a 495 será "bajas", de 495 a 1593 será "intermedia", de 1593 a 7820 será "altas" y superior a 7820 será "muy altas".
```{r}
summary(df$VENTAS)
```

```{r}
df$nventas <- cut(df$"VENTAS", breaks = c(0, 495, 1593, 7820, Inf), labels = c("Ventas bajas", "Ventas intermedias", "Ventas altas", "Ventas muy altas"))
```
La tabla de contingencia es la que se muestra a continuación. Podemos ver como las microempresas tienen en mayor medida las ventas bajas e intermedias lo cuál es normal pues al ser una empresa con menos empleados se supone que serán tiendas pequeñas que venden menos que las empresas con más empleados. Aún así, 513 microempresas tienen ventas altas y 50 tienen ventas muy altas. Las empresas pequeñas tienen unos valores más lógicos pues 23 de ellas tienen ventas bajas, 179 tienen ventas intermedias y el resto se distribuye entre altas y muy altas (582 y 717). La empresa mediana no tiene ventas bajas y más del 97% de ellas tienen ventas muy altas. Por último, cabe destacar que las grandes empresas no tienen ventas lo cual da que pensar que no hay grandes empresas en este dataframe.
```{r}
table(df$empresa,df$nventas)
```
  
Confirmamos que no existen grandes empresas en este dataframe:
```{r}
summary(df$empresa)
```



## Ejercicio 4

Comparar las ventas entre Madrid y Barcelona. 
  
Para ello vamos a separar las ventas de Madrid y Barcelona:

```{r}
ventas4 <- filter(df, PROVINCIA=="Madrid" | PROVINCIA=="Barcelona")
ventas4
```
Para ver qué distribuciones tienen haremos un diagrama de cajas y bigotes. Aparentemente ambas distribuciones son muy parecidas. Sin embargo, tenemos que comprobarlo no visualmente sino formulando las hipótesis y realizando el test correspondiente. Para ello, crearemos dos nuevos dataframes con los datos exclusivamente de Madrid y de Barcelona.
```{r}
ventasM<- filter(df, PROVINCIA=="Madrid")
ventasB<- filter(df, PROVINCIA=="Barcelona")
par(mfrow = c(1, 2))
boxplot(ventasM$VENTAS)
title("Ventas Madrid")
boxplot(ventasB$VENTAS)
title("Ventas Barcelona")

```



Realizamos el test de  Mann-Whitney U ya que tenemos la variable ventas que es continua y no normal y la variable PROVINCIAS (donde solamente tenemos Madrid y Barcelona) que es categórica. La hipótesis nula será que tienen la misma media y por tanto son iguales. Como p es mayor que 0.05 no podemos rechazar la hipótesis nula y por tanto afirmamos que las ventas de Madrid y Barcelona son de igual media y no son significativas entre ellas.

```{r}
wilcox.test(VENTAS ~ PROVINCIA, ventas4)
```
  
De igual manera, podemos reutilizar el apartado anterior para ver la distribución de ventas y empleados en ambas ciudades y observamos que son distribuciones muy similares:


```{r}
table(ventasM$empresa,ventasM$nventas)
```
```{r}
table(ventasB$empresa,ventasB$nventas)
```


## Ejercicio 5

Presentación del modelo de predicción de las ventas para el siguiente año y describirla adecuadamente.

Para ello, lo primero será eliminar las columnas creadas en el apartado 3 y separar los datos en train y test.
```{r}
df<- df %>% 
   dplyr::select(-c(nventas, empresa))
```
```{r}
set.seed(123)
split_inicial <- initial_split(
                    data   = df,
                    prop   = 0.8,
                    strata = VENTAS
                 )
datos_train <- training(split_inicial)
datos_test  <- testing(split_inicial)
```

Usaremos en una primera aproximación un modelo de regresión lineal con todas las variables:
```{r}
full.model<- lm(VENTAS ~ .,data=datos_train)
summary(full.model)
```

Con ello podemos ver como las variables que afectan más al modelo serán las que tengan más asteriscos *. Calculamos las ventas predichas y calculamos los errores.
```{r}

preds <- predict(full.model, newdata = datos_test, type = "response")


```
Los errores a analizar son el RMSE, r^2 y MAE:
```{r}
err1<-postResample(preds, datos_test$VENTAS)
err1
```
Podemos ver que el Rsquared está próximo a uno por lo que los datos se ajustan bien a la recta pero los errores son muy grandes. Vamos a analizar los residuos para la validación del modelo y consta de 4 supuestos que son los siguientes:
```{r}
plot(full.model)
```

a) El gráfico de residuals vs fitted nos indica si hay homocedasticidad, es decir que la varianza es constante. En él observamos que existe una tendencia de los puntos por lo que aparentemente no hay homocedasticidad en los residuos. Lo confirmamos con el test de Breusch-Pagan donde la hipótesis nula es que la varianza es constante y con el valor de p menor que 0.05 rechazamos dicha hipótesis y afirmamos que no hay varianza constante:

```{r}
library(lmtest)
bptest(full.model)
```

b) El gráfico normal Q-Q indica la normalidad de los residuos y para que haya los puntos deben seguir la recta (en nuestro caso no la siguen). Para confirmarlo haremos el test de shapiro y vemos que rechazamos la hipótesis nula por lo que no existe normalidad en los residuos.

```{r}
shapiro.test(full.model$residuals)
```
 
c) El gráfico de scale-location explica la independencia de los residuos y para ello la línea roja debe ser próxima a una recta. En nuestro caso no parece que haya dependencia por lo que vemos el test de Durbin-watson, cuyo resultado si el valor está entre 1.5 y 2.5 puede concluirse que no existe dependencia de los residuos.

```{r}
dwtest(full.model)
```

d)Por último, tenemos que ver si existe linealidad o no en los residuos pero al tener un R squared superior a 0.7 se puede afirmar que los residuos son lineales.  


Para ver si mejoramos el modelo calculamos el modelo lineal pero aplicando el método stepwise con esto se descartarán las variables redundantes de información y menos significativas para la predicción:

```{r}
step.model <- stepAIC(full.model, direction = "both", 
                      trace = TRUE)
summary(step.model)
```
El nuevo modelo dependerá por tanto de las variables PROVINCIA, PRODUCTIVIDAD, NÚMERO DE EMPLEADOS, coe, conce, numac, numest y estp. Calculamos los datos predecidos:

```{r}

preds2 <- predict(step.model, newdata = datos_test, type = "response")


```
Los errores son muy parecidos al full model pero se ahorra en variables:
```{r}
err2<-postResample(preds2, datos_test$VENTAS)
err2
```

Podemos ver que el Rsquared está próximo a uno por lo que los datos se ajustan bien a la recta pero los errores son muy grandes. Vamos a analizar los residuos para la validación del modelo:
```{r}
plot(step.model)
```

a) En el gráfico de residuals vs fitted observamos que existe una tendencia de los puntos por lo que aparentemente no hay homocedasticidad en los residuos. Lo confirmamos con el test de Breusch-Pagan donde la hipótesis nula es que la varianza es constante y con el valor de p menor que 0.05 rechazamos dicha hipótesis y afirmamos que no hay varianza constante:

```{r}
library(lmtest)
bptest(step.model)
```

b) El gráfico normal Q-Q indica que no hay normalidad. Para confirmarlo haremos el test de shapiro y vemos que rechazamos la hipótesis nula por lo que no existe normalidad de nuevo en los residuos.

```{r}
shapiro.test(step.model$residuals)
```
 
c) En el gráfico de scale-location no parece que haya dependencia por lo que vemos el test de Durbin-watson:

```{r}
dwtest(step.model)
```

d)Por último, tenemos que ver si existe linealidad o no en los residuos pero al tener un R squared superior a 0.7 se puede afirmar que los residuos son lineales.  



Como hemos hecho predicciones a través de modelos lineales probaremos a predecir los datos del siguiente año a través del uso de un random forest que calcula una combinación de árboles predictores y elige el que mejor se ajuste a dichos datos:

```{r}
modelo_rf <- rand_forest(
                 mode  = "regression",
                 mtry  = tune(),
                 trees = tune(),
                 min_n = tune()
              ) %>%
              set_engine(engine = "ranger")

# DEFINICIÓN DEL PREPROCESADO
# =============================================================================
transformer <- recipe(
                  formula = VENTAS ~ .,
                  data =  datos_train
               ) %>%
               step_naomit(all_predictors()) %>%
               step_nzv(all_predictors()) %>%
               step_center(all_numeric(), -all_outcomes()) %>%
               step_scale(all_numeric(), -all_outcomes()) %>%
               step_dummy(all_nominal(), -all_outcomes())

# DEFINICIÓN DE LA ESTRATEGIA DE VALIDACIÓN Y CREACIÓN DE PARTICIONES
# =============================================================================
set.seed(1234)
cv_folds <- vfold_cv(
              data    = datos_train,
              v       = 5,
              strata  = VENTAS
             )

# WORKFLOW
# =============================================================================
workflow_modelado <- workflow() %>%
                     add_recipe(transformer) %>%
                     add_model(modelo_rf)

# GRID DE HIPERPARÁMETROS
# =============================================================================
hiperpar_grid <- grid_max_entropy(
                  # Rango de búsqueda para cada hiperparámetro
                  mtry(range = c(1L, 10L), trans = NULL),
                  trees(range = c(10L, 30L), trans = NULL),
                  min_n(range = c(2L, 100L), trans = NULL),
                  # Número de combinaciones totales
                  size = 100
                )

# EJECUCIÓN DE LA OPTIMIZACIÓN DE HIPERPARÁMETROS
# =============================================================================
registerDoParallel(cores = parallel::detectCores() - 1)
grid_fit <- tune_grid(
              object    = workflow_modelado,
              resamples = cv_folds,
              metrics   = metric_set(rmse),
              control   = control_resamples(save_pred = TRUE),
              # Hiperparámetros
              grid      = hiperpar_grid
            )
stopImplicitCluster()
```

Mostramos los mejores modelos que hay según el rmse:
```{r}
show_best(grid_fit, metric = "rmse")
```

Entrenamos los datos con el modelo elegido:
```{r}
mejores_hiperpar <- select_best(grid_fit, metric = "rmse")

modelo_rf <- finalize_workflow(
                x = workflow_modelado,
                parameters = mejores_hiperpar
             )

modelo_rf_fit <- modelo_rf %>%
                 fit(
                  data = datos_train
                 )
```

Predecimos los datos del test:
```{r}
preds3 <- modelo_rf_fit %>%
                predict(
                  new_data = datos_test,
                  type     = "numeric"
                )
```

Calculamos los errores de las ventas predichas y podemos ver que el RMSE ha bajado y el Rsquared ha subido considerablemente por lo que este modelo predice mejor que los dos anteriores:
```{r}
err3<-postResample(preds3, datos_test$VENTAS)
err3
```



Por último, podemos intentar crear un modelo a partir de las variables que consideramos significativas gracias al estudio estadístico previo que hemos realizado. Consideraremos que la variable PROVINCIA no es significativa por lo que la descartamos y que al tener alta correlación entre numac y numest una de ellas la podemos descartar.

```{r}
rmodel <- lm(VENTAS ~ coe + conce + numac +`NÚMERO DE EMPLEADOS`+ PRODUCTIVIDAD , data = datos_train)
summary(rmodel)
```


```{r}

preds4 <- predict(rmodel, newdata = datos_test, type = "response")


```
Los errores son muy parecidos al full model:
```{r}
err4<-postResample(preds4, datos_test$VENTAS)
err4
```
```{r}
plot(rmodel)
```
 
El último modelo predice de peor forma que los demás pero un valor de 0.81 ajusta considerablemente bien los datos y la validación en los residuos resulta equivalente a las dos validaciones anteriores por lo que se puede concluir que los 3 modelos lineales son muy parecidos y si queremos un modelo que prediga mejor las ventas podemos usar un random forest que es más completo en este caso. Por tanto, las ventas del próximo año serán:

```{r}
preds3
```


esto es una prueba para github2



